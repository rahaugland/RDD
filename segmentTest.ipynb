{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 358 layers, 312342832 parameters, 312342832 gradients\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\Desktop\\data\\RDD\\AutoAnnotation\\..\\Norway\\train\\images\\Norway_000545.jpg: 1024x1024 458.0ms\n",
      "Speed: 6.0ms preprocess, 458.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import SAM, YOLO\n",
    "\n",
    "# Load a model\n",
    "model = SAM('sam_l.pt')\n",
    "\n",
    "# Display model information (optional)\n",
    "model.info()\n",
    "\n",
    "# Run inference with bboxes prompt\n",
    "result = model(\"../Norway/train/images/Norway_000545.jpg\",points=[1100,400])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Results object with attributes:\n",
       "\n",
       "boxes: None\n",
       "keypoints: None\n",
       "masks: ultralytics.engine.results.Masks object\n",
       "names: {0: '0'}\n",
       "orig_img: array([[[244, 241, 236],\n",
       "        [241, 238, 233],\n",
       "        [239, 236, 231],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [254, 254, 254],\n",
       "        [  3,   3,   3]],\n",
       "\n",
       "       [[244, 241, 236],\n",
       "        [242, 239, 234],\n",
       "        [240, 237, 232],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [254, 254, 254],\n",
       "        [  3,   3,   3]],\n",
       "\n",
       "       [[245, 242, 237],\n",
       "        [243, 240, 235],\n",
       "        [242, 239, 234],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [254, 254, 254],\n",
       "        [  3,   3,   3]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 65,  66,  70],\n",
       "        [ 64,  65,  69],\n",
       "        [ 63,  64,  68],\n",
       "        ...,\n",
       "        [124, 126, 127],\n",
       "        [126, 128, 129],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[ 65,  66,  70],\n",
       "        [ 64,  65,  69],\n",
       "        [ 63,  64,  68],\n",
       "        ...,\n",
       "        [124, 126, 127],\n",
       "        [126, 128, 129],\n",
       "        [  0,   0,   0]],\n",
       "\n",
       "       [[ 66,  67,  71],\n",
       "        [ 65,  66,  70],\n",
       "        [ 63,  64,  68],\n",
       "        ...,\n",
       "        [118, 120, 121],\n",
       "        [121, 123, 124],\n",
       "        [  0,   0,   0]]], dtype=uint8)\n",
       "orig_shape: (1225, 3643)\n",
       "path: 'c:\\\\Users\\\\idi40\\\\Desktop\\\\data\\\\RDD\\\\AutoAnnotation\\\\..\\\\Norway\\\\train\\\\images\\\\Norway_000512.jpg'\n",
       "probs: None\n",
       "save_dir: None\n",
       "speed: {'preprocess': 3.000020980834961, 'inference': 154.00123596191406, 'postprocess': 1.0001659393310547}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1227, 3650)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_bounding_box(mask):\n",
    "    # Convert tensor to numpy array if it's a PyTorch tensor\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "    print(mask.shape)\n",
    "    # Get the dimensions of the mask\n",
    "    _ , height, width = mask.shape\n",
    "    \n",
    "    # Initialize extreme points of the bounding box\n",
    "    min_x, min_y = width, height\n",
    "    max_x, max_y = 0, 0\n",
    "    \n",
    "    # Iterate through each pixel in the mask\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            if mask[0, y, x] == True:  # Assuming True indicates the object in the mask\n",
    "                # Update extreme points of the bounding box\n",
    "                min_x = min(min_x, x)\n",
    "                min_y = min(min_y, y)\n",
    "                max_x = max(max_x, x)\n",
    "                max_y = max(max_y, y)\n",
    "    \n",
    "    # If no True pixels found, return None\n",
    "    if min_x == width or min_y == height or max_x == 0 or max_y == 0:\n",
    "        print(\"No 'True' pixels found in the mask.\")\n",
    "        return None\n",
    "    \n",
    "    # Compute width and height of the bounding box\n",
    "    width_bb = max_x - min_x + 1\n",
    "    height_bb = max_y - min_y + 1\n",
    "    \n",
    "    return min_x, min_y, width_bb, height_bb\n",
    "\n",
    "bbox = get_bounding_box(result[0].masks.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 236, 3580, 991)\n"
     ]
    }
   ],
   "source": [
    "img_path = \"../Norway/train/images/Norway_000545.jpg\"\n",
    "import cv2\n",
    "print(bbox)\n",
    "def crop_image(image_path, min_x, min_y, max_x, max_y):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Crop the image using the bounding box coordinates\n",
    "    cropped_image = image[min_y:max_y, min_x:max_x]\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "min_x, min_y, width, height = bbox\n",
    "cropped_image = crop_image(img_path,min_x, min_y, min_x+width, min_y+height)\n",
    "\n",
    "# Display the cropped image\n",
    "cv2.imshow(\"Cropped Image\", cropped_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x1408 6 D00s, 2 D20s, 75.8ms\n",
      "Speed: 4.0ms preprocess, 75.8ms inference, 4.0ms postprocess per image at shape (1, 3, 416, 1408)\n",
      "Results saved to \u001b[1mruns\\detect\\predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "yolo =  YOLO(\"../runs/detect/train31/weights/best.pt\")\n",
    "result = yolo.predict(cropped_image, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
