{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 0.16.1\n",
      "2.1.1\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "print(\"Torch version:\", torchvision.__version__)\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation from the paper \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "from torchvision.transforms import v2 as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "\n",
    "class RoadDamageDataset(Dataset):\n",
    "     def __init__(self, root, annFile, transform=None, target_transform=None, include_img_without_cracks=True):\n",
    "        \n",
    "        self.root = root\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.crackless = include_img_without_cracks\n",
    "\n",
    "     def __len__(self):\n",
    "           return len(self.ids)\n",
    "     \n",
    "     def __getitem__(self, index):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "        targets = {}\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        area = []\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        img = read_image(os.path.join(self.root, path))\n",
    "        img = tv_tensors.Image(img)\n",
    "        iscrowd = []\n",
    "        #img = img.resize((1221, 4040))\n",
    "        #img = tv_tensors.Image(img)\n",
    "        for t in target:\n",
    "            x_min, y_min, width, height = t[\"bbox\"]\n",
    "            x_max, y_max = x_min + width, y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(t[\"category_id\"])\n",
    "            area.append(t[\"area\"])\n",
    "            iscrowd.append(t[\"iscrowd\"])\n",
    "        if len(target) == 0:\n",
    "            if self.crackless:\n",
    "                boxes.append([0,0,1,1])\n",
    "                area.append(0)\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                return \n",
    "        targets[\"image_id\"] = torch.as_tensor(index)\n",
    "        targets[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
    "        targets[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        targets[\"area\"] = torch.as_tensor(area)\n",
    "\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img, targets= self.transform(img, targets)\n",
    "        \n",
    "          \n",
    "        \n",
    "\n",
    "        return img, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.10s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "annFile = '../coco_annotations.json'\n",
    "root = \"train/images/\"\n",
    "annFile_val = '../coco_annotations_validation.json'\n",
    "root_val = \"validate/images/\"\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "road_damage_dataset = RoadDamageDataset(root, annFile, transform=get_transform(train=True), target_transform=None)\n",
    "road_damage_dataset_validate = RoadDamageDataset(root_val, annFile_val, transform=get_transform(train=False), target_transform=None )\n",
    "mini_dataset_size = 50  # Set the desired size for your mini-dataset\n",
    "\n",
    "\n",
    "# Create a random split to get a mini-dataset\n",
    "mini_dataset, _ = random_split(road_damage_dataset, [mini_dataset_size, len(road_damage_dataset) - mini_dataset_size])\n",
    "mini_dataset_val, _ = random_split(road_damage_dataset, [mini_dataset_size, len(road_damage_dataset) - mini_dataset_size])\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = data_loader = torch.utils.data.DataLoader(\n",
    "    road_damage_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = data_loader = torch.utils.data.DataLoader(\n",
    "    road_damage_dataset_validate,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    mini_dataset,\n",
    "    batch_size=5,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "data_loader_1 = torch.utils.data.DataLoader(\n",
    "    mini_dataset_val,\n",
    "    batch_size=5,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(img) = <class 'torch.Tensor'>\n",
      "type(target) = <class 'dict'>\n",
      "target.keys() = dict_keys(['image_id', 'boxes', 'labels', 'area'])\n",
      "type(target['boxes']) = <class 'torch.Tensor'>\n",
      "type(target['labels']) = <class 'torch.Tensor'>\n",
      "tensor([[ 514.0000,  811.6000,  641.0000,  922.6000],\n",
      "        [  84.0000,  924.6000,  420.0000, 1217.6000],\n",
      "        [1765.0000,  597.6000, 2318.0000, 1210.6000],\n",
      "        [1652.0000,  626.6000, 1741.0000,  700.6000]])\n"
     ]
    }
   ],
   "source": [
    "sample = road_damage_dataset[10]\n",
    "img, target = sample\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")\n",
    "print(target[\"boxes\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 5  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/2]  eta: 0:00:01  lr: 0.005000  loss: 2.7486 (2.7486)  loss_classifier: 2.0895 (2.0895)  loss_box_reg: 0.0073 (0.0073)  loss_objectness: 0.4339 (0.4339)  loss_rpn_box_reg: 0.2179 (0.2179)  time: 0.6960  data: 0.0750  max mem: 3758\n",
      "Epoch: [0]  [1/2]  eta: 0:00:00  lr: 0.005000  loss: 2.7486 (4.4105)  loss_classifier: 2.0895 (2.1162)  loss_box_reg: 0.0073 (0.0110)  loss_objectness: 0.4339 (1.5499)  loss_rpn_box_reg: 0.2179 (0.7333)  time: 0.4815  data: 0.0840  max mem: 3918\n",
      "Epoch: [0] Total time: 0:00:00 (0.4815 s / it)\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "\n",
    "model.to(device)\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "# let's train it just for 2 epochs\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    print(\"1\")\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    print(\"2\")\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Trainig Loss: 1.0686, Validation Loss: 0.2544\n",
      "Best model saved.\n",
      "Epoch [2/250], Trainig Loss: 0.2452, Validation Loss: 0.2330\n",
      "Best model saved.\n",
      "Epoch [3/250], Trainig Loss: 0.2315, Validation Loss: 0.2205\n",
      "Best model saved.\n",
      "Epoch [4/250], Trainig Loss: 0.2165, Validation Loss: 0.2113\n",
      "Best model saved.\n",
      "Epoch [5/250], Trainig Loss: 0.2116, Validation Loss: 0.2236\n",
      "Epoch [6/250], Trainig Loss: 0.2091, Validation Loss: 0.1919\n",
      "Best model saved.\n",
      "Epoch [7/250], Trainig Loss: 0.2075, Validation Loss: 0.2078\n",
      "Epoch [8/250], Trainig Loss: 0.2065, Validation Loss: 0.1883\n",
      "Best model saved.\n",
      "Epoch [9/250], Trainig Loss: 0.2059, Validation Loss: 0.1919\n",
      "Epoch [10/250], Trainig Loss: 0.2037, Validation Loss: 0.2182\n",
      "Model saved at epoch 10.\n",
      "Epoch [11/250], Trainig Loss: 0.2013, Validation Loss: 0.2004\n",
      "Epoch [12/250], Trainig Loss: 0.2013, Validation Loss: 0.1854\n",
      "Best model saved.\n",
      "Epoch [13/250], Trainig Loss: 0.2174, Validation Loss: 0.2623\n",
      "Epoch [14/250], Trainig Loss: 98390.6094, Validation Loss: 2.2287\n",
      "Epoch [15/250], Trainig Loss: 1.9376, Validation Loss: 1.4920\n",
      "Epoch [16/250], Trainig Loss: 1.5488, Validation Loss: 1.2857\n",
      "Epoch [17/250], Trainig Loss: 1.3890, Validation Loss: 1.1715\n",
      "Epoch [18/250], Trainig Loss: 1.2920, Validation Loss: 1.1124\n",
      "Epoch [19/250], Trainig Loss: 1.2131, Validation Loss: 1.0523\n",
      "Epoch [20/250], Trainig Loss: 1.1477, Validation Loss: 1.0090\n",
      "Model saved at epoch 20.\n",
      "Epoch [21/250], Trainig Loss: 1.0909, Validation Loss: 0.9678\n",
      "Epoch [22/250], Trainig Loss: 1.0447, Validation Loss: 0.9427\n",
      "Epoch [23/250], Trainig Loss: 1.0075, Validation Loss: 0.9201\n",
      "Epoch [24/250], Trainig Loss: 8.3674, Validation Loss: 0.9087\n",
      "Epoch [25/250], Trainig Loss: 0.9767, Validation Loss: 0.9111\n",
      "Epoch [26/250], Trainig Loss: 0.9748, Validation Loss: 0.8959\n",
      "Epoch [27/250], Trainig Loss: 0.9619, Validation Loss: 0.9091\n",
      "Epoch [28/250], Trainig Loss: 109.4047, Validation Loss: 0.8995\n",
      "Epoch [29/250], Trainig Loss: 0.9510, Validation Loss: 0.8904\n",
      "Epoch [30/250], Trainig Loss: 0.9444, Validation Loss: 0.8769\n",
      "Model saved at epoch 30.\n",
      "Epoch [31/250], Trainig Loss: 0.9399, Validation Loss: 0.8928\n",
      "Epoch [32/250], Trainig Loss: 0.9371, Validation Loss: 0.8853\n",
      "Epoch [33/250], Trainig Loss: 0.9339, Validation Loss: 0.8696\n",
      "Epoch [34/250], Trainig Loss: 0.9310, Validation Loss: 0.8889\n",
      "Epoch [35/250], Trainig Loss: 0.9285, Validation Loss: 0.8682\n",
      "Epoch [36/250], Trainig Loss: 0.9280, Validation Loss: 0.8779\n",
      "Epoch [37/250], Trainig Loss: 0.9271, Validation Loss: 0.8656\n",
      "Epoch [38/250], Trainig Loss: 0.9272, Validation Loss: 0.8621\n",
      "Epoch [39/250], Trainig Loss: 0.9262, Validation Loss: 0.8630\n",
      "Epoch [40/250], Trainig Loss: 0.9261, Validation Loss: 0.8649\n",
      "Model saved at epoch 40.\n",
      "Epoch [41/250], Trainig Loss: 0.9275, Validation Loss: 0.8826\n",
      "Epoch [42/250], Trainig Loss: 0.9267, Validation Loss: 0.8869\n",
      "Epoch [43/250], Trainig Loss: 0.9265, Validation Loss: 0.8643\n",
      "Epoch [44/250], Trainig Loss: 0.9278, Validation Loss: 0.8790\n",
      "Epoch [45/250], Trainig Loss: 0.9265, Validation Loss: 0.8651\n",
      "Epoch [46/250], Trainig Loss: 0.9266, Validation Loss: 0.8631\n",
      "Epoch [47/250], Trainig Loss: 0.9260, Validation Loss: 0.8756\n",
      "Epoch [48/250], Trainig Loss: 247.9883, Validation Loss: 0.8648\n",
      "Epoch [49/250], Trainig Loss: 0.9262, Validation Loss: 0.8702\n",
      "Epoch [50/250], Trainig Loss: 0.9270, Validation Loss: 0.8724\n",
      "Model saved at epoch 50.\n",
      "Epoch [51/250], Trainig Loss: 0.9269, Validation Loss: 0.8629\n",
      "Epoch [52/250], Trainig Loss: 0.9260, Validation Loss: 0.8751\n",
      "Epoch [53/250], Trainig Loss: 0.9263, Validation Loss: 0.8622\n",
      "Epoch [54/250], Trainig Loss: 0.9280, Validation Loss: 0.8949\n",
      "Epoch [55/250], Trainig Loss: 756.0109, Validation Loss: 0.8719\n",
      "Epoch [56/250], Trainig Loss: 0.9276, Validation Loss: 0.8866\n",
      "Epoch [57/250], Trainig Loss: 0.9267, Validation Loss: 0.8630\n",
      "Epoch [58/250], Trainig Loss: 0.9267, Validation Loss: 0.8661\n",
      "Epoch [59/250], Trainig Loss: 0.9267, Validation Loss: 0.8637\n",
      "Epoch [60/250], Trainig Loss: 0.9265, Validation Loss: 0.8636\n",
      "Model saved at epoch 60.\n",
      "Epoch [61/250], Trainig Loss: 0.9258, Validation Loss: 0.8675\n",
      "Epoch [62/250], Trainig Loss: 0.9267, Validation Loss: 0.8629\n",
      "Epoch [63/250], Trainig Loss: 0.9274, Validation Loss: 0.8660\n",
      "Epoch [64/250], Trainig Loss: 0.9265, Validation Loss: 0.8746\n",
      "Epoch [65/250], Trainig Loss: 0.9271, Validation Loss: 0.8739\n",
      "Epoch [66/250], Trainig Loss: 0.9261, Validation Loss: 0.8618\n",
      "Epoch [67/250], Trainig Loss: 0.9264, Validation Loss: 0.8728\n",
      "Epoch [68/250], Trainig Loss: 0.9267, Validation Loss: 0.9058\n",
      "Epoch [69/250], Trainig Loss: 0.9259, Validation Loss: 0.8840\n",
      "Epoch [70/250], Trainig Loss: 0.9269, Validation Loss: 0.8662\n",
      "Model saved at epoch 70.\n",
      "Epoch [71/250], Trainig Loss: 0.9269, Validation Loss: 0.8689\n",
      "Epoch [72/250], Trainig Loss: 0.9277, Validation Loss: 0.8636\n",
      "Epoch [73/250], Trainig Loss: 0.9275, Validation Loss: 0.8625\n",
      "Epoch [74/250], Trainig Loss: 0.9267, Validation Loss: 0.8752\n",
      "Epoch [75/250], Trainig Loss: 0.9257, Validation Loss: 0.8656\n",
      "Epoch [76/250], Trainig Loss: 0.9270, Validation Loss: 0.8620\n",
      "Epoch [77/250], Trainig Loss: 0.9269, Validation Loss: 0.8646\n",
      "Epoch [78/250], Trainig Loss: 0.9267, Validation Loss: 0.8625\n",
      "Epoch [79/250], Trainig Loss: 0.9276, Validation Loss: 0.8624\n",
      "Epoch [80/250], Trainig Loss: 0.9268, Validation Loss: 0.8622\n",
      "Model saved at epoch 80.\n",
      "Epoch [81/250], Trainig Loss: 0.9279, Validation Loss: 0.8635\n",
      "Epoch [82/250], Trainig Loss: 0.9269, Validation Loss: 0.8854\n",
      "Epoch [83/250], Trainig Loss: 0.9282, Validation Loss: 0.8893\n",
      "Epoch [84/250], Trainig Loss: 0.9266, Validation Loss: 0.8890\n",
      "Epoch [85/250], Trainig Loss: 0.9263, Validation Loss: 0.8629\n",
      "Epoch [86/250], Trainig Loss: 0.9265, Validation Loss: 0.8645\n",
      "Epoch [87/250], Trainig Loss: 0.9263, Validation Loss: 0.8838\n",
      "Epoch [88/250], Trainig Loss: 0.9266, Validation Loss: 0.8848\n",
      "Epoch [89/250], Trainig Loss: 0.9270, Validation Loss: 0.8634\n",
      "Epoch [90/250], Trainig Loss: 0.9259, Validation Loss: 0.8631\n",
      "Model saved at epoch 90.\n",
      "Epoch [91/250], Trainig Loss: 0.9272, Validation Loss: 0.8618\n",
      "Epoch [92/250], Trainig Loss: 0.9262, Validation Loss: 0.8813\n",
      "Epoch [93/250], Trainig Loss: 0.9263, Validation Loss: 0.8732\n",
      "Epoch [94/250], Trainig Loss: 0.9277, Validation Loss: 0.8848\n",
      "Epoch [95/250], Trainig Loss: 0.9278, Validation Loss: 0.8705\n",
      "Epoch [96/250], Trainig Loss: 0.9270, Validation Loss: 0.8679\n",
      "Epoch [97/250], Trainig Loss: 0.9268, Validation Loss: 0.8695\n",
      "Epoch [98/250], Trainig Loss: 0.9267, Validation Loss: 0.8632\n",
      "Epoch [99/250], Trainig Loss: 0.9263, Validation Loss: 0.8872\n",
      "Epoch [100/250], Trainig Loss: 0.9266, Validation Loss: 0.8628\n",
      "Model saved at epoch 100.\n",
      "Epoch [101/250], Trainig Loss: 0.9271, Validation Loss: 0.8634\n",
      "Epoch [102/250], Trainig Loss: 0.9255, Validation Loss: 0.8664\n",
      "Epoch [103/250], Trainig Loss: 0.9272, Validation Loss: 0.8708\n",
      "Epoch [104/250], Trainig Loss: 0.9271, Validation Loss: 0.8856\n",
      "Epoch [105/250], Trainig Loss: 0.9264, Validation Loss: 0.8842\n",
      "Epoch [106/250], Trainig Loss: 0.9274, Validation Loss: 0.8697\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch import Tensor, tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from itertools import islice\n",
    "save_path = \"model\"\n",
    "# Set up loss function, optimizer, and device\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Move the model to the specified device\n",
    "model.to(device)\n",
    "def train_model(model, num_epochs, train_loader, val_loader, criterion, optimizer, save_model=False, save_plots=False):\n",
    "    best_loss = 100\n",
    "    train_losses = []\n",
    "    val_batch_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        i = 0    \n",
    "        for imgs, annotations in train_loader:\n",
    "            i += 1\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    \n",
    "            loss_dict = model(imgs, annotations)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            epoch_loss += losses\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(epoch_loss/len(train_loader))\n",
    "            \n",
    "            \n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for val_imgs, val_annotations in val_loader:\n",
    "                val_imgs = list(img.to(device) for img in val_imgs)\n",
    "                val_annotations = [{k: v.to(device) for k, v in t.items()} for t in val_annotations]\n",
    "                val_loss_dict = model(val_imgs, val_annotations)\n",
    "                val_losses = sum(val_loss for val_loss in val_loss_dict.values())\n",
    "                val_loss += val_losses\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_batch_loss.append(avg_val_loss)\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Trainig Loss: {epoch_loss/len(train_loader):.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            if save_model:\n",
    "            # Save the best model\n",
    "                if avg_val_loss < best_loss:\n",
    "                    best_loss = avg_val_loss\n",
    "                    torch.save(model.state_dict(), save_path+\"best.pth\")\n",
    "                    print(\"Best model saved.\")\n",
    "\n",
    "            # Save the model every 10 epochs\n",
    "            if (epoch + 1) % 10 == 0 and save_model:\n",
    "                torch.save(model.state_dict(), f\"model_epoch_{epoch + 1}.pth\")\n",
    "                print(f\"Model saved at epoch {epoch + 1}.\")\n",
    "        if save_plots and (epoch + 1) % 10 == 0:\n",
    "            # Save loss plot\n",
    "            val_batch_loss_np = np.array([val.cpu().numpy() for val in val_batch_loss])\n",
    "            train_batch_loss_np = np.array([train.detach().cpu().numpy() for train in train_losses])\n",
    "            plt.plot(train_batch_loss_np, label='Training Loss')\n",
    "            plt.plot(val_batch_loss_np, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'loss_plot_epoch_{epoch + 1}.png')\n",
    "            plt.close()\n",
    "                \n",
    "                # Save training progress to a text file\n",
    "            with open(\"training_log.txt\", \"a\") as f:\n",
    "                f.write(f'Epoch [{epoch + 1}/{num_epochs}], Total Loss: {epoch_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\\n')\n",
    "\n",
    "train_model(model, 250, train_loader, val_loader, criterion, optimizer, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as torchtrans  \n",
    "\n",
    "def apply_nms(orig_prediction, iou_thresh=0.5):\n",
    "    \n",
    "    # torchvision returns the indices of the bboxes to keep\n",
    "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
    "    \n",
    "    final_prediction = orig_prediction\n",
    "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
    "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
    "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
    "    \n",
    "    return final_prediction\n",
    "\n",
    "def get_good_pred(orig_prediction, t=0.3):\n",
    "    final_pred = apply_nms(orig_prediction)\n",
    "    good_pred =[]\n",
    "    if len(final_pred) == 0 :\n",
    "        return []\n",
    "    for i in range(len(final_pred[\"boxes\"])):\n",
    "        if final_pred[\"scores\"][i] > t:\n",
    "             good_pred.append({\n",
    "                \"boxes\": final_pred[\"boxes\"][i],\n",
    "                \"scores\": final_pred[\"scores\"][i],\n",
    "                \"labels\": final_pred[\"labels\"][i]\n",
    "            })\n",
    "    return good_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m draw_bounding_boxes, draw_segmentation_masks\n\u001b[0;32m      4\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m model_finetuned \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mdetection\u001b[38;5;241m.\u001b[39mfasterrcnn_resnet50_fpn(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Assuming the model was trained on COCO dataset\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Replace the classifier with a new one, with the appropriate number of classes\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.utils import draw_bounding_boxes, draw_segmentation_masks\n",
    "i = 0\n",
    "model_finetuned = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "num_classes = 5  # Assuming the model was trained on COCO dataset\n",
    "# Replace the classifier with a new one, with the appropriate number of classes\n",
    "in_features = model_finetuned.roi_heads.box_predictor.cls_score.in_features\n",
    "model_finetuned.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"modelbest.pth\")\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model_finetuned.load_state_dict(state_dict)\n",
    "model_finetuned.to(device)\n",
    "model_finetuned.eval()\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    image = read_image(f\"train/images/Norway_000{501+i}.jpg\")\n",
    "    image_pil = Image.open(f\"train/images/Norway_000{501+i}.jpg\")\n",
    "    eval_transform = get_transform(train=False)\n",
    "    # Convert PIL image to a NumPy array\n",
    "    image_np = np.array(image_pil)\n",
    "    with torch.no_grad():\n",
    "        x = eval_transform(image)\n",
    "        # convert RGBA -> RGB and move to device\n",
    "        x = x[:3, ...].to(device)\n",
    "        predictions = model_finetuned([x, ])\n",
    "        \n",
    "        pred = predictions[0]\n",
    "    \n",
    "    if len(pred[\"boxes\"] > 0):\n",
    "        image = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\n",
    "        image = image[:3, ...]\n",
    "        pred_labels = [f\"{label}: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "        pred_boxes = pred[\"boxes\"]\n",
    "        filtered_labels = []\n",
    "        filtered_boxes = []\n",
    "        filtered_scores = []\n",
    "        for label, score, box in zip(pred[\"labels\"], pred[\"scores\"], pred[\"boxes\"]):\n",
    "            if score >= 0.3:\n",
    "                filtered_labels.append(f\"{label}: {score:.3f}\")\n",
    "                filtered_boxes.append(box.unsqueeze(0))\n",
    "                filtered_scores.append(score)\n",
    "        if (len(filtered_boxes) > 0):\n",
    "            filtered_boxes = torch.cat(filtered_boxes, dim=0)\n",
    "            filtered_scores = [tensor.unsqueeze(0) for tensor in filtered_scores]\n",
    "            filtered_scores = torch.cat(filtered_scores, dim=0)\n",
    "\n",
    "            keep = torchvision.ops.nms(filtered_boxes, filtered_scores.to(device), iou_threshold=0.2)\n",
    "            boxes_to_plot = torch.cat([filtered_boxes[i].unsqueeze(0) for i in keep], dim=0)\n",
    "            labels_to_plot = [filtered_labels[i] for i in keep]\n",
    "            output_image = draw_bounding_boxes(image, boxes_to_plot, labels_to_plot, colors=\"red\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            plt.figure(figsize=(12, 12))\n",
    "            plt.imshow(output_image.permute(1, 2, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
