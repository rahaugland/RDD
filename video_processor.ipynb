{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, SAM\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_bounding_box(mask):\n",
    "    # Convert tensor to numpy array if it's a PyTorch tensor\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.cpu().numpy()\n",
    "   \n",
    "    _ , height, width = mask.shape  \n",
    "    min_x, min_y = width, height\n",
    "    max_x, max_y = 0, 0\n",
    "    \n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            if mask[0, y, x] == True:  \n",
    "                min_x = min(min_x, x)\n",
    "                min_y = min(min_y, y)\n",
    "                max_x = max(max_x, x)\n",
    "                max_y = max(max_y, y)\n",
    "    \n",
    "    # If no True pixels found, return None\n",
    "    if min_x == width or min_y == height or max_x == 0 or max_y == 0:\n",
    "        print(\"No 'True' pixels found in the mask.\")\n",
    "        return None\n",
    "     \n",
    "    width_bb = max_x - min_x + 1\n",
    "    height_bb = max_y - min_y + 1\n",
    "    \n",
    "    return min_x, min_y, width_bb, height_bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(image_path, min_x, min_y, max_x, max_y):\n",
    "    image = cv2.imread(image_path)\n",
    "    cropped_image = image[min_y:max_y, min_x:max_x]\n",
    "    return cropped_image\n",
    "\n",
    "def extract_frames(video_path, output_folder, focus_point, auto_crop=False):\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success, image = vidcap.read()\n",
    "    count = 0\n",
    "    prefix = video_path.split(\".\")[0]\n",
    "    total_frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    num_digits = len(str(total_frames))\n",
    "    if auto_crop:\n",
    "        sam = SAM('sam_l.pt')\n",
    "        cropped_output_folder = f\"{output_folder}_cropped\"\n",
    "        while success:\n",
    "            filename = f\"{prefix}_{count:0{num_digits}}.jpg\"\n",
    "            cv2.imwrite(output_folder + \"/\" + filename, image)  # save frame as JPEG file\n",
    "            result = sam(output_folder + \"/\" + filename, points=focus_point)\n",
    "            bbox = get_bounding_box(result[0].masks.data)\n",
    "            min_x, min_y, width, height = bbox\n",
    "            cropped_image = crop_image(output_folder + \"/\" + filename,min_x, min_y, min_x+width, min_y+height)\n",
    "            cv2.imwrite(f\"{cropped_output_folder}/{filename}\", cropped_image)\n",
    "            success, image = vidcap.read()\n",
    "            print('Saved frame %s' % filename)\n",
    "            count += 1\n",
    "    else: \n",
    "         while success:\n",
    "            filename = f\"{prefix}_{count:0{num_digits}}\"\n",
    "            cv2.imwrite(output_folder + \"/\" + filename, image)  # save frame as JPEG file\n",
    "            success, image = vidcap.read()\n",
    "            print('Saved frame %s' % filename)\n",
    "            count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images(image_folder, output_folder, focus_point):\n",
    "    sam = SAM('sam_l.pt')\n",
    "    image_files = os.listdir(image_folder)\n",
    "    image_paths = [os.path.join(image_folder, file) for file in image_files]\n",
    "    for path in image_paths:\n",
    "        \n",
    "        filename = path.split(\"\\\\\")[-1]\n",
    "        filename = filename.split(\".\")[0]\n",
    "        \n",
    "        result = sam(path, points=focus_point)\n",
    "        bbox = get_bounding_box(result[0].masks.data)\n",
    "        min_x, min_y, width, height = bbox\n",
    "        cropped_image = crop_image(path,min_x, min_y, min_x+width, min_y+height)\n",
    "        print(f\"{output_folder}/{filename}\")\n",
    "        cv2.imwrite(f\"{output_folder}/{filename}.jpg\", cropped_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_video(video_path, save_dir_images, project, sub_project, focus_point, auto_crop=False):\n",
    "    extract_frames(video_path=video_path, output_folder=save_dir_images,focus_point=focus_point, auto_crop=auto_crop)\n",
    "    yolo =  YOLO(\"../runs/detect/train31/weights/best.pt\")\n",
    "    \n",
    "    if auto_crop:\n",
    "        image_files = os.listdir(f\"{save_dir_images}_cropped\")\n",
    "        image_paths = [os.path.join(f\"{save_dir_images}_cropped\", file) for file in image_files]\n",
    "    else:\n",
    "        image_file = os.listdir(save_dir_images)\n",
    "        image_paths = [os.path.join(save_dir_images, file) for file in image_files]\n",
    "\n",
    "    print(image_paths)\n",
    "    results = yolo(image_paths, stream=True, save_txt=True, project=project, name=sub_project)\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Boxes object for bbox outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_YOLO = \"best.pt\"\n",
    "def process_folder(data_folder, output_folder, project, sub_project,focus_point, auto_crop=False):\n",
    "    yolo =  YOLO(PATH_YOLO)\n",
    "    if auto_crop:\n",
    "        crop_images(data_folder, output_folder=f\"{output_folder}_cropped\", focus_point=focus_point)\n",
    "    if auto_crop:\n",
    "        image_files = os.listdir(f\"{output_folder}_cropped\")\n",
    "        image_paths = [os.path.join(f\"{output_folder}_cropped\", file) for file in image_files]\n",
    "    else:\n",
    "        image_files = os.listdir(data_folder)\n",
    "        image_paths = [os.path.join(data_folder, file) for file in image_files]\n",
    "\n",
    "    \n",
    "    results = yolo(image_paths, stream=True, save_txt=True, project=project, name=sub_project)\n",
    "    for result in results:\n",
    "        boxes = result.boxes  # Boxes object for bbox outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000501.jpg: 1024x1024 1020.4ms\n",
      "Speed: 75.0ms preprocess, 1020.4ms inference, 13.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000501\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000502.jpg: 1024x1024 281.0ms\n",
      "Speed: 4.0ms preprocess, 281.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000502\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000503.jpg: 1024x1024 334.0ms\n",
      "Speed: 4.0ms preprocess, 334.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000503\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000504.jpg: 1024x1024 323.0ms\n",
      "Speed: 4.0ms preprocess, 323.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000504\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000505.jpg: 1024x1024 330.0ms\n",
      "Speed: 4.0ms preprocess, 330.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000505\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000506.jpg: 1024x1024 296.0ms\n",
      "Speed: 4.0ms preprocess, 296.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000506\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000507.jpg: 1024x1024 316.0ms\n",
      "Speed: 3.0ms preprocess, 316.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000507\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000508.jpg: 1024x1024 271.0ms\n",
      "Speed: 3.0ms preprocess, 271.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000508\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000509.jpg: 1024x1024 254.0ms\n",
      "Speed: 3.0ms preprocess, 254.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000509\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000510.jpg: 1024x1024 259.0ms\n",
      "Speed: 4.0ms preprocess, 259.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000510\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000511.jpg: 1024x1024 331.0ms\n",
      "Speed: 3.0ms preprocess, 331.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000511\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000512.jpg: 1024x1024 321.0ms\n",
      "Speed: 4.0ms preprocess, 321.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000512\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000513.jpg: 1024x1024 288.0ms\n",
      "Speed: 3.0ms preprocess, 288.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000513\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000514.jpg: 1024x1024 322.0ms\n",
      "Speed: 5.0ms preprocess, 322.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000514\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000515.jpg: 1024x1024 326.0ms\n",
      "Speed: 3.0ms preprocess, 326.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000515\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000516.jpg: 1024x1024 319.0ms\n",
      "Speed: 4.0ms preprocess, 319.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000516\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000517.jpg: 1024x1024 324.0ms\n",
      "Speed: 4.0ms preprocess, 324.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000517\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000518.jpg: 1024x1024 255.9ms\n",
      "Speed: 4.0ms preprocess, 255.9ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000518\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000519.jpg: 1024x1024 318.0ms\n",
      "Speed: 4.0ms preprocess, 318.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000519\n",
      "\n",
      "image 1/1 c:\\Users\\idi40\\RDD\\RDD\\test_images\\Norway_000520.jpg: 1024x1024 267.0ms\n",
      "Speed: 4.0ms preprocess, 267.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "images_test_cropped/Norway_000520\n",
      "\n",
      "0: 1408x1408 (no detections), 1: 1408x1408 (no detections), 2: 1408x1408 (no detections), 3: 1408x1408 (no detections), 4: 1408x1408 (no detections), 5: 1408x1408 (no detections), 6: 1408x1408 (no detections), 7: 1408x1408 (no detections), 8: 1408x1408 2 D00s, 9: 1408x1408 (no detections), 10: 1408x1408 3 D00s, 11: 1408x1408 6 D00s, 3 D10s, 12: 1408x1408 (no detections), 13: 1408x1408 2 D00s, 14: 1408x1408 2 D00s, 15: 1408x1408 (no detections), 16: 1408x1408 (no detections), 17: 1408x1408 (no detections), 18: 1408x1408 (no detections), 19: 1408x1408 6 D00s, 1 D10, 20: 1408x1408 3 D00s, 1 D20, 21: 1408x1408 (no detections), 22: 1408x1408 10 D00s, 23: 1408x1408 1 D00, 24: 1408x1408 (no detections), 25: 1408x1408 (no detections), 26: 1408x1408 (no detections), 27: 1408x1408 (no detections), 28: 1408x1408 3 D00s, 29: 1408x1408 (no detections), 30: 1408x1408 (no detections), 294.5ms\n",
      "Speed: 8.5ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1408, 1408)\n",
      "Results saved to \u001b[1mAutoAnnotation\\auto_labels2\u001b[0m\n",
      "10 labels saved to AutoAnnotation\\auto_labels2\\labels\n"
     ]
    }
   ],
   "source": [
    "PROJECT = \"AutoAnnotation\"\n",
    "SUB_PROJECT = \"auto_labels\"\n",
    "VIDEO_PATH = \"testy.mp4\"\n",
    "test_vid = \"TEST.mp4\"\n",
    "OUTPUT_FOLDER = \"images_test\"\n",
    "FOCUS_POINT = [1100,1400]\n",
    "\n",
    "process_folder(data_folder=\"test_images\",\n",
    "                output_folder=OUTPUT_FOLDER, \n",
    "                project=PROJECT, \n",
    "                sub_project=SUB_PROJECT,\n",
    "                focus_point=FOCUS_POINT,\n",
    "                auto_crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/sam_l.pt to 'sam_l.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.16G/1.16G [00:28<00:00, 43.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "images_test/TEST_00.jpg does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m process_video(video_path\u001b[38;5;241m=\u001b[39mtest_vid, \n\u001b[0;32m      2\u001b[0m               save_dir_images\u001b[38;5;241m=\u001b[39mOUTPUT_FOLDER,\n\u001b[0;32m      3\u001b[0m               project\u001b[38;5;241m=\u001b[39mPROJECT, \n\u001b[0;32m      4\u001b[0m               sub_project\u001b[38;5;241m=\u001b[39mSUB_PROJECT,\n\u001b[0;32m      5\u001b[0m               focus_point\u001b[38;5;241m=\u001b[39mFOCUS_POINT, \n\u001b[0;32m      6\u001b[0m               auto_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, save_dir_images, project, sub_project, focus_point, auto_crop)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_video\u001b[39m(video_path, save_dir_images, project, sub_project, focus_point, auto_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     extract_frames(video_path\u001b[38;5;241m=\u001b[39mvideo_path, output_folder\u001b[38;5;241m=\u001b[39msave_dir_images,focus_point\u001b[38;5;241m=\u001b[39mfocus_point, auto_crop\u001b[38;5;241m=\u001b[39mauto_crop)\n\u001b[0;32m      3\u001b[0m     yolo \u001b[38;5;241m=\u001b[39m  YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../runs/detect/train31/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m auto_crop:\n",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m, in \u001b[0;36mextract_frames\u001b[1;34m(video_path, output_folder, focus_point, auto_crop)\u001b[0m\n\u001b[0;32m     17\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_digits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename, image)  \u001b[38;5;66;03m# save frame as JPEG file\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m result \u001b[38;5;241m=\u001b[39m sam(output_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename, points\u001b[38;5;241m=\u001b[39mfocus_point)\n\u001b[0;32m     20\u001b[0m bbox \u001b[38;5;241m=\u001b[39m get_bounding_box(result[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmasks\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m     21\u001b[0m min_x, min_y, width, height \u001b[38;5;241m=\u001b[39m bbox\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\models\\sam\\model.py:92\u001b[0m, in \u001b[0;36mSAM.__call__\u001b[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, bboxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    Alias for the 'predict' method.\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m        (list): The model predictions.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, bboxes, points, labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\models\\sam\\model.py:76\u001b[0m, in \u001b[0;36mSAM.predict\u001b[1;34m(self, source, stream, bboxes, points, labels, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(overrides)\n\u001b[0;32m     75\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(bboxes\u001b[38;5;241m=\u001b[39mbboxes, points\u001b[38;5;241m=\u001b[39mpoints, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(source, stream, prompts\u001b[38;5;241m=\u001b[39mprompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:239\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:198\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_source(source \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\models\\sam\\predict.py:388\u001b[0m, in \u001b[0;36mPredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03mSets up the data source for inference.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    source (str | Path): The path to the image data source for inference.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msetup_source(source)\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:215\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransforms\u001b[39m\u001b[38;5;124m'\u001b[39m, classify_transforms(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz[\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassify\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m load_inference_source(source\u001b[38;5;241m=\u001b[39msource,\n\u001b[0;32m    216\u001b[0m                                      imgsz\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz,\n\u001b[0;32m    217\u001b[0m                                      vid_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvid_stride,\n\u001b[0;32m    218\u001b[0m                                      buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mstream_buffer)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  \u001b[38;5;66;03m# streams\u001b[39;00m\n\u001b[0;32m    221\u001b[0m                                           \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m  \u001b[38;5;66;03m# images\u001b[39;00m\n\u001b[0;32m    222\u001b[0m                                           \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\data\\build.py:172\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, imgsz, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    170\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadPilAndNumpy(source, imgsz\u001b[38;5;241m=\u001b[39mimgsz)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadImages(source, imgsz\u001b[38;5;241m=\u001b[39mimgsz, vid_stride\u001b[38;5;241m=\u001b[39mvid_stride)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Attach source types to the dataset\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28msetattr\u001b[39m(dataset, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_type\u001b[39m\u001b[38;5;124m'\u001b[39m, source_type)\n",
      "File \u001b[1;32mc:\\Users\\idi40\\anaconda3\\Lib\\site-packages\\ultralytics\\data\\loaders.py:292\u001b[0m, in \u001b[0;36mLoadImages.__init__\u001b[1;34m(self, path, imgsz, vid_stride)\u001b[0m\n\u001b[0;32m    290\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m((parent \u001b[38;5;241m/\u001b[39m p)\u001b[38;5;241m.\u001b[39mabsolute()))  \u001b[38;5;66;03m# files (relative to *.txt file parent)\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    294\u001b[0m images \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m IMG_FORMATS]\n\u001b[0;32m    295\u001b[0m videos \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m VID_FORMATS]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: images_test/TEST_00.jpg does not exist"
     ]
    }
   ],
   "source": [
    "process_video(video_path=test_vid, \n",
    "              save_dir_images=OUTPUT_FOLDER,\n",
    "              project=PROJECT, \n",
    "              sub_project=SUB_PROJECT,\n",
    "              focus_point=FOCUS_POINT, \n",
    "              auto_crop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
